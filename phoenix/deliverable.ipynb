{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "884427c3",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Hypothesis Generation\n",
    "\n",
    "This notebook is adapted from various files in `./notebooks` and contains the following modifications:\n",
    "1. **OpenAI $\\rightarrow$ Gemini API**. GPT is not cheap! I opted for Google's free Tier I plan.\n",
    "2. **Annotation by Embedding Space Metrics**. I explored performing hypothesis annotation on the holdout set using two metrics on the embedding space: cosine similarity and distance.\n",
    "3. **`SupervisedSparseAutoencoder` Class**. I tried my hand at implementing the class specified in this [feature request](https://github.com/rmovva/HypotheSAEs/issues/2). I was able to implement the constructor and forward pass (to the best of my knowledge) before running out of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "291b1715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "dotenv_path = find_dotenv()\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5' # Set to the index of the GPU you want to use; see visible GPUs with `nvidia-smi` on command line\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from hypothesaes.quickstart import train_sae, interpret_sae, generate_hypotheses, evaluate_hypotheses\n",
    "from hypothesaes.embedding import get_local_embeddings\n",
    "from hypothesaes.llm_local import get_vllm_engine\n",
    "from hypothesaes.select_neurons import select_neurons\n",
    "from hypothesaes.interpret_neurons import NeuronInterpreter, SamplingConfig, LLMConfig, InterpretConfig, ScoringConfig\n",
    "from hypothesaes.annotate import annotate_texts_with_concepts\n",
    "from hypothesaes.evaluation import score_hypotheses\n",
    "from hypothesaes.sae import SparseAutoencoder\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "assert current_dir.endswith(\"phoenix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073234cd",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The dataset I will be using is [Twitter Tweet Sentiments (27.5k)](https://www.kaggle.com/datasets/yasserh/twitter-tweets-sentiment-dataset), a collection of 27,480 tweets and the associated sentiments. 5496 of these tweets are used for validation during SAE training and 5496 tweets used for holdout evaluation. The target variable is the `sentiment` column, which can be `negative`, `neutral`, or `positive`, and we are interested in seeing what features of the `text` column predict it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f6fccc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19413</th>\n",
       "      <td>ba7bbe76fe</td>\n",
       "      <td>: saw it yesterday. Pretty good.</td>\n",
       "      <td>good.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23147</th>\n",
       "      <td>09b5bef434</td>\n",
       "      <td>hey, I can`t make it to Makers tonight</td>\n",
       "      <td>hey, I can`t make it to Makers tonight</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21547</th>\n",
       "      <td>83cdebaa92</td>\n",
       "      <td>Whats with you though, you sound a bit down y...</td>\n",
       "      <td>you sound a bit down yourself.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14162</th>\n",
       "      <td>914da6164e</td>\n",
       "      <td>No B2G1 for me.  Trying to save cash for next ...</td>\n",
       "      <td>No B2G1 for me.  Trying to save cash for next ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6474</th>\n",
       "      <td>d7709f9f53</td>\n",
       "      <td>hahahaha omg you win the internetz today!  'W...</td>\n",
       "      <td>a omg you win</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "19413  ba7bbe76fe                   : saw it yesterday. Pretty good.   \n",
       "23147  09b5bef434             hey, I can`t make it to Makers tonight   \n",
       "21547  83cdebaa92   Whats with you though, you sound a bit down y...   \n",
       "14162  914da6164e  No B2G1 for me.  Trying to save cash for next ...   \n",
       "6474   d7709f9f53   hahahaha omg you win the internetz today!  'W...   \n",
       "\n",
       "                                           selected_text  sentiment  \n",
       "19413                                              good.          1  \n",
       "23147             hey, I can`t make it to Makers tonight          0  \n",
       "21547                     you sound a bit down yourself.         -1  \n",
       "14162  No B2G1 for me.  Trying to save cash for next ...          0  \n",
       "6474                                       a omg you win          1  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = os.path.join(\"data\")\n",
    "df = pd.read_csv(os.path.join(base_dir, \"Tweets.csv\"))\n",
    "df = df[pd.notnull(df[\"text\"])] # GPT generated command to clean :)\n",
    "\n",
    "sentiment_to_numeric = {\"negative\" : -1, \"neutral\" : 0, \"positive\" : 1}\n",
    "df[\"sentiment\"] = df[\"sentiment\"].map(sentiment_to_numeric) # We need numeric labels for regression!\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=5496*2, train_size=16448, random_state=42)\n",
    "val_df_SAE, val_df_holdout = train_test_split(val_df, test_size=5496, train_size=5496, random_state=42)\n",
    "\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a5a7c5",
   "metadata": {},
   "source": [
    "## 1. Feature Generation\n",
    "First, we will compute the embeddings of the `text` column for the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "68d04300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7656b4d7eb742a48ebab619e59d34a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading embedding chunks:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27460 embeddings in 0.1s\n"
     ]
    }
   ],
   "source": [
    "train_texts = train_df[\"text\"].tolist()\n",
    "train_sentiments = train_df[\"sentiment\"].tolist()\n",
    "val_texts = val_df_SAE[\"text\"].tolist()\n",
    "\n",
    "EMBEDDER = \"nomic-ai/modernbert-embed-base\"\n",
    "CACHE_NAME = f\"twitter_quickstart_local_{EMBEDDER}\"\n",
    "\n",
    "text2embedding = get_local_embeddings(train_texts + val_texts, model=EMBEDDER, batch_size=128, cache_name=CACHE_NAME)\n",
    "train_embeddings = np.stack([text2embedding[text] for text in train_texts])\n",
    "val_embeddings = np.stack([text2embedding[text] for text in val_texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c19e9f",
   "metadata": {},
   "source": [
    "Now that we have the embeddings, we will use a SAE wiil sparsify these representations. Since the size of the dataset used here and the dataset used in `quickstart_local.ipynb` are relatively similar, and after consulting the `README`, I decided to train a Matryoshka SAE with the same parameters $M=256$, $k=8$, and $\\text{prefix\\_lengths} = [32, 256]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "af5b6dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from data/checkpoints/twitter_quickstart_local_nomic-ai/modernbert-embed-base/SAE_matryoshka_M=256_K=8_prefixes=32-256.pt onto device cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626b2a87e051492fb1165f19fe8f6f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing activations (batchsize=16384):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_dir = os.path.join(base_dir, \"checkpoints\", CACHE_NAME)\n",
    "sae = train_sae(embeddings=train_embeddings, M=256, K=8, matryoshka_prefix_lengths=[32, 256], checkpoint_dir=checkpoint_dir, val_embeddings=val_embeddings)\n",
    "train_activations = sae.get_activations(train_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8719a27",
   "metadata": {},
   "source": [
    "## 2. Feature Selection\n",
    "Now that we have the sparse activations of our training embeddings, we select the neurons which are most predictive of `sentiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "94b922a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_method = \"correlation\"\n",
    "top_neuron_count = 20\n",
    "\n",
    "selected_neurons, scores = select_neurons(\n",
    "    activations=train_activations,\n",
    "    target=train_sentiments,\n",
    "    n_select=top_neuron_count,\n",
    "    method=selection_method,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284fc5f",
   "metadata": {},
   "source": [
    "## 3. Feature Interpretation\n",
    "By Proposition 3.1, a natural language concept is predictive of the target variable if it is predictive of activations of a neuron that is predictive of that variable. Since we have found the neurons which are predictive of `sentiment`, it suffices to find the concepts which are predictive of each neuron's activation.\n",
    "\n",
    "\n",
    "To do so, we leverage the [Gemini API](https://googleapis.github.io/python-genai) to interpret the neurons with natural language concepts via LLM. Fortunately, `NeuronInterpreter` does a lot of the heavy lifting required for this. Unfortunately, it does not do all of the heavy lifting. The following adjustments must be made:\n",
    "- **Annotator and Interpreter Model**. Currently, only the OpenAI API is supported. To get around this, I rehashed `llm_api.py` and made the appropriate changes in the `_execute_prompts` and `_get_interpretation_gemini` (formerly known as `_get_interpretation_openai`) methods needed for `NeuronInterpreter.interpret_neurons`. Later, annotation required minor changes in `annotate.py`.\n",
    "\n",
    "- **Rate Limiting**. Initially, I was using the standard plan and rate limiting was a huge issue. The fix was to scale almost everything down. I settled on rolling back the parallelization and opted for executing the prompts sequentially, along with cutting down the number of examples used to score the fidelity of each neuron interpretation. Eventually, I was able to get a better plan that allowed for much better throughput, essentially making these changes mostly obsolete (but still worth mentioning)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3f39b587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83818cad517540519ea59812d300d6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating interpretations:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TASK_SPECIFIC_INSTRUCTIONS = \"\"\"All of the texts are tweets. \n",
    "Features should describe a specific aspect of the tweet. For example:\n",
    "- \"mentions excitement about a sports event\"\n",
    "- \"uses humor to comment on politics\"\n",
    "- \"praises a new tech product release\"\n",
    "- \"complains about poor customer service from a company\"\n",
    "\"\"\"\n",
    "\n",
    "interpreter = NeuronInterpreter(\n",
    "    interpreter_model=\"gemini-2.5-flash-lite\",\n",
    "    annotator_model=\"gemini-2.5-flash-lite\",\n",
    "    n_workers_interpretation=10,\n",
    "    n_workers_annotation=50,\n",
    "    cache_name=CACHE_NAME,\n",
    ")\n",
    "\n",
    "interpret_config = InterpretConfig(\n",
    "    sampling=SamplingConfig(\n",
    "        n_examples=20,\n",
    "        max_words_per_example=128,\n",
    "    ),\n",
    "    llm=LLMConfig(\n",
    "        temperature=0.7,\n",
    "        max_interpretation_tokens=75,\n",
    "    ),\n",
    "    n_candidates=3,\n",
    "    task_specific_instructions=TASK_SPECIFIC_INSTRUCTIONS,\n",
    ")\n",
    "\n",
    "interpretations = interpreter.interpret_neurons(\n",
    "    texts=train_texts,\n",
    "    activations=train_activations,\n",
    "    neuron_indices=selected_neurons,\n",
    "    config=interpret_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed486c",
   "metadata": {},
   "source": [
    "After obtaining the interpretations, we score each interpretation by how correlated it is with neuron activation. In the paper, this is referred to as fidelity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "61720596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1950 cached items; annotating 1050 uncached items\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9865d93a73747f188907a0f890c9c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring neuron interpretation fidelity (20 neurons; 3 candidate interps per neuron; 50 examples to score each …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_06eb6\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_06eb6_level0_col0\" class=\"col_heading level0 col0\" >neuron_idx</th>\n",
       "      <th id=\"T_06eb6_level0_col1\" class=\"col_heading level0 col1\" >target_correlation</th>\n",
       "      <th id=\"T_06eb6_level0_col2\" class=\"col_heading level0 col2\" >best_interpretation</th>\n",
       "      <th id=\"T_06eb6_level0_col3\" class=\"col_heading level0 col3\" >best_f1</th>\n",
       "      <th id=\"T_06eb6_level0_col4\" class=\"col_heading level0 col4\" >worst_interpretation</th>\n",
       "      <th id=\"T_06eb6_level0_col5\" class=\"col_heading level0 col5\" >worst_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_06eb6_row0_col0\" class=\"data row0 col0\" >28</td>\n",
       "      <td id=\"T_06eb6_row0_col1\" class=\"data row0 col1\" >0.343308</td>\n",
       "      <td id=\"T_06eb6_row0_col2\" class=\"data row0 col2\" >expresses strong positive emotion</td>\n",
       "      <td id=\"T_06eb6_row0_col3\" class=\"data row0 col3\" >0.89</td>\n",
       "      <td id=\"T_06eb6_row0_col4\" class=\"data row0 col4\" >expresses strong positive sentiment</td>\n",
       "      <td id=\"T_06eb6_row0_col5\" class=\"data row0 col5\" >0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "      <td id=\"T_06eb6_row1_col0\" class=\"data row1 col0\" >27</td>\n",
       "      <td id=\"T_06eb6_row1_col1\" class=\"data row1 col1\" >0.243332</td>\n",
       "      <td id=\"T_06eb6_row1_col2\" class=\"data row1 col2\" >expresses gratitude</td>\n",
       "      <td id=\"T_06eb6_row1_col3\" class=\"data row1 col3\" >0.98</td>\n",
       "      <td id=\"T_06eb6_row1_col4\" class=\"data row1 col4\" >expresses gratitude</td>\n",
       "      <td id=\"T_06eb6_row1_col5\" class=\"data row1 col5\" >0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
       "      <td id=\"T_06eb6_row2_col0\" class=\"data row2 col0\" >5</td>\n",
       "      <td id=\"T_06eb6_row2_col1\" class=\"data row2 col1\" >0.191838</td>\n",
       "      <td id=\"T_06eb6_row2_col2\" class=\"data row2 col2\" >mentions Mother's Day or mothers</td>\n",
       "      <td id=\"T_06eb6_row2_col3\" class=\"data row2 col3\" >1.00</td>\n",
       "      <td id=\"T_06eb6_row2_col4\" class=\"data row2 col4\" >mentions Mother's Day or mothers</td>\n",
       "      <td id=\"T_06eb6_row2_col5\" class=\"data row2 col5\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row3\" class=\"row_heading level0 row3\" >6</th>\n",
       "      <td id=\"T_06eb6_row3_col0\" class=\"data row3 col0\" >12</td>\n",
       "      <td id=\"T_06eb6_row3_col1\" class=\"data row3 col1\" >0.140774</td>\n",
       "      <td id=\"T_06eb6_row3_col2\" class=\"data row3 col2\" >uses an exclamation mark at the end of the tweet</td>\n",
       "      <td id=\"T_06eb6_row3_col3\" class=\"data row3 col3\" >0.86</td>\n",
       "      <td id=\"T_06eb6_row3_col4\" class=\"data row3 col4\" >uses exclamation marks</td>\n",
       "      <td id=\"T_06eb6_row3_col5\" class=\"data row3 col5\" >0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row4\" class=\"row_heading level0 row4\" >10</th>\n",
       "      <td id=\"T_06eb6_row4_col0\" class=\"data row4 col0\" >243</td>\n",
       "      <td id=\"T_06eb6_row4_col1\" class=\"data row4 col1\" >0.100437</td>\n",
       "      <td id=\"T_06eb6_row4_col2\" class=\"data row4 col2\" >expresses love towards another person</td>\n",
       "      <td id=\"T_06eb6_row4_col3\" class=\"data row4 col3\" >0.82</td>\n",
       "      <td id=\"T_06eb6_row4_col4\" class=\"data row4 col4\" >expresses love or affection</td>\n",
       "      <td id=\"T_06eb6_row4_col5\" class=\"data row4 col5\" >0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row5\" class=\"row_heading level0 row5\" >14</th>\n",
       "      <td id=\"T_06eb6_row5_col0\" class=\"data row5 col0\" >21</td>\n",
       "      <td id=\"T_06eb6_row5_col1\" class=\"data row5 col1\" >0.088182</td>\n",
       "      <td id=\"T_06eb6_row5_col2\" class=\"data row5 col2\" >mentions creating or promoting content/products</td>\n",
       "      <td id=\"T_06eb6_row5_col3\" class=\"data row5 col3\" >0.92</td>\n",
       "      <td id=\"T_06eb6_row5_col4\" class=\"data row5 col4\" >mentions a website or a link to a website</td>\n",
       "      <td id=\"T_06eb6_row5_col5\" class=\"data row5 col5\" >0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row6\" class=\"row_heading level0 row6\" >18</th>\n",
       "      <td id=\"T_06eb6_row6_col0\" class=\"data row6 col0\" >249</td>\n",
       "      <td id=\"T_06eb6_row6_col1\" class=\"data row6 col1\" >0.080873</td>\n",
       "      <td id=\"T_06eb6_row6_col2\" class=\"data row6 col2\" >uses the word 'morning'</td>\n",
       "      <td id=\"T_06eb6_row6_col3\" class=\"data row6 col3\" >1.00</td>\n",
       "      <td id=\"T_06eb6_row6_col4\" class=\"data row6 col4\" >uses the word 'morning'</td>\n",
       "      <td id=\"T_06eb6_row6_col5\" class=\"data row6 col5\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row7\" class=\"row_heading level0 row7\" >19</th>\n",
       "      <td id=\"T_06eb6_row7_col0\" class=\"data row7 col0\" >136</td>\n",
       "      <td id=\"T_06eb6_row7_col1\" class=\"data row7 col1\" >0.078432</td>\n",
       "      <td id=\"T_06eb6_row7_col2\" class=\"data row7 col2\" >uses the word 'cute' to describe someone or something</td>\n",
       "      <td id=\"T_06eb6_row7_col3\" class=\"data row7 col3\" >1.00</td>\n",
       "      <td id=\"T_06eb6_row7_col4\" class=\"data row7 col4\" >uses the word 'cute'</td>\n",
       "      <td id=\"T_06eb6_row7_col5\" class=\"data row7 col5\" >0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row8\" class=\"row_heading level0 row8\" >17</th>\n",
       "      <td id=\"T_06eb6_row8_col0\" class=\"data row8 col0\" >178</td>\n",
       "      <td id=\"T_06eb6_row8_col1\" class=\"data row8 col1\" >-0.081368</td>\n",
       "      <td id=\"T_06eb6_row8_col2\" class=\"data row8 col2\" >mentions discomfort in the stomach</td>\n",
       "      <td id=\"T_06eb6_row8_col3\" class=\"data row8 col3\" >0.94</td>\n",
       "      <td id=\"T_06eb6_row8_col4\" class=\"data row8 col4\" >mentions discomfort in the stomach</td>\n",
       "      <td id=\"T_06eb6_row8_col5\" class=\"data row8 col5\" >0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row9\" class=\"row_heading level0 row9\" >16</th>\n",
       "      <td id=\"T_06eb6_row9_col0\" class=\"data row9 col0\" >242</td>\n",
       "      <td id=\"T_06eb6_row9_col1\" class=\"data row9 col1\" >-0.083918</td>\n",
       "      <td id=\"T_06eb6_row9_col2\" class=\"data row9 col2\" >expresses sadness</td>\n",
       "      <td id=\"T_06eb6_row9_col3\" class=\"data row9 col3\" >0.83</td>\n",
       "      <td id=\"T_06eb6_row9_col4\" class=\"data row9 col4\" >expresses sadness</td>\n",
       "      <td id=\"T_06eb6_row9_col5\" class=\"data row9 col5\" >0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row10\" class=\"row_heading level0 row10\" >15</th>\n",
       "      <td id=\"T_06eb6_row10_col0\" class=\"data row10 col0\" >85</td>\n",
       "      <td id=\"T_06eb6_row10_col1\" class=\"data row10 col1\" >-0.084001</td>\n",
       "      <td id=\"T_06eb6_row10_col2\" class=\"data row10 col2\" >mentions a physical ailment of the head</td>\n",
       "      <td id=\"T_06eb6_row10_col3\" class=\"data row10 col3\" >1.00</td>\n",
       "      <td id=\"T_06eb6_row10_col4\" class=\"data row10 col4\" >mentions physical discomfort or illness</td>\n",
       "      <td id=\"T_06eb6_row10_col5\" class=\"data row10 col5\" >0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row11\" class=\"row_heading level0 row11\" >13</th>\n",
       "      <td id=\"T_06eb6_row11_col0\" class=\"data row11 col0\" >32</td>\n",
       "      <td id=\"T_06eb6_row11_col1\" class=\"data row11 col1\" >-0.093657</td>\n",
       "      <td id=\"T_06eb6_row11_col2\" class=\"data row11 col2\" >expresses a state of being tired</td>\n",
       "      <td id=\"T_06eb6_row11_col3\" class=\"data row11 col3\" >0.94</td>\n",
       "      <td id=\"T_06eb6_row11_col4\" class=\"data row11 col4\" >expresses fatigue or tiredness</td>\n",
       "      <td id=\"T_06eb6_row11_col5\" class=\"data row11 col5\" >0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_06eb6_row12_col0\" class=\"data row12 col0\" >116</td>\n",
       "      <td id=\"T_06eb6_row12_col1\" class=\"data row12 col1\" >-0.096747</td>\n",
       "      <td id=\"T_06eb6_row12_col2\" class=\"data row12 col2\" >mentions the word 'missed'</td>\n",
       "      <td id=\"T_06eb6_row12_col3\" class=\"data row12 col3\" >0.89</td>\n",
       "      <td id=\"T_06eb6_row12_col4\" class=\"data row12 col4\" >mentions missing an event or opportunity</td>\n",
       "      <td id=\"T_06eb6_row12_col5\" class=\"data row12 col5\" >0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row13\" class=\"row_heading level0 row13\" >11</th>\n",
       "      <td id=\"T_06eb6_row13_col0\" class=\"data row13 col0\" >244</td>\n",
       "      <td id=\"T_06eb6_row13_col1\" class=\"data row13 col1\" >-0.097314</td>\n",
       "      <td id=\"T_06eb6_row13_col2\" class=\"data row13 col2\" >uses the word 'sucks'</td>\n",
       "      <td id=\"T_06eb6_row13_col3\" class=\"data row13 col3\" >0.96</td>\n",
       "      <td id=\"T_06eb6_row13_col4\" class=\"data row13 col4\" >uses the word 'sucks'</td>\n",
       "      <td id=\"T_06eb6_row13_col5\" class=\"data row13 col5\" >0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row14\" class=\"row_heading level0 row14\" >9</th>\n",
       "      <td id=\"T_06eb6_row14_col0\" class=\"data row14 col0\" >140</td>\n",
       "      <td id=\"T_06eb6_row14_col1\" class=\"data row14 col1\" >-0.105862</td>\n",
       "      <td id=\"T_06eb6_row14_col2\" class=\"data row14 col2\" >uses the word 'bored' or variations thereof</td>\n",
       "      <td id=\"T_06eb6_row14_col3\" class=\"data row14 col3\" >1.00</td>\n",
       "      <td id=\"T_06eb6_row14_col4\" class=\"data row14 col4\" >expresses boredom</td>\n",
       "      <td id=\"T_06eb6_row14_col5\" class=\"data row14 col5\" >0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row15\" class=\"row_heading level0 row15\" >8</th>\n",
       "      <td id=\"T_06eb6_row15_col0\" class=\"data row15 col0\" >42</td>\n",
       "      <td id=\"T_06eb6_row15_col1\" class=\"data row15 col1\" >-0.117070</td>\n",
       "      <td id=\"T_06eb6_row15_col2\" class=\"data row15 col2\" >expresses apology</td>\n",
       "      <td id=\"T_06eb6_row15_col3\" class=\"data row15 col3\" >1.00</td>\n",
       "      <td id=\"T_06eb6_row15_col4\" class=\"data row15 col4\" >expresses apology</td>\n",
       "      <td id=\"T_06eb6_row15_col5\" class=\"data row15 col5\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row16\" class=\"row_heading level0 row16\" >7</th>\n",
       "      <td id=\"T_06eb6_row16_col0\" class=\"data row16 col0\" >251</td>\n",
       "      <td id=\"T_06eb6_row16_col1\" class=\"data row16 col1\" >-0.127503</td>\n",
       "      <td id=\"T_06eb6_row16_col2\" class=\"data row16 col2\" >expresses strong negative emotion, specifically hate or dislike</td>\n",
       "      <td id=\"T_06eb6_row16_col3\" class=\"data row16 col3\" >0.90</td>\n",
       "      <td id=\"T_06eb6_row16_col4\" class=\"data row16 col4\" >expresses strong negative emotion using the word 'hate'</td>\n",
       "      <td id=\"T_06eb6_row16_col5\" class=\"data row16 col5\" >0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row17\" class=\"row_heading level0 row17\" >5</th>\n",
       "      <td id=\"T_06eb6_row17_col0\" class=\"data row17 col0\" >10</td>\n",
       "      <td id=\"T_06eb6_row17_col1\" class=\"data row17 col1\" >-0.141938</td>\n",
       "      <td id=\"T_06eb6_row17_col2\" class=\"data row17 col2\" >expresses a feeling of longing or missing someone or something</td>\n",
       "      <td id=\"T_06eb6_row17_col3\" class=\"data row17 col3\" >0.96</td>\n",
       "      <td id=\"T_06eb6_row17_col4\" class=\"data row17 col4\" >expresses longing or missing someone/something</td>\n",
       "      <td id=\"T_06eb6_row17_col5\" class=\"data row17 col5\" >0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row18\" class=\"row_heading level0 row18\" >4</th>\n",
       "      <td id=\"T_06eb6_row18_col0\" class=\"data row18 col0\" >8</td>\n",
       "      <td id=\"T_06eb6_row18_col1\" class=\"data row18 col1\" >-0.177269</td>\n",
       "      <td id=\"T_06eb6_row18_col2\" class=\"data row18 col2\" >mentions being sick or having symptoms of illness</td>\n",
       "      <td id=\"T_06eb6_row18_col3\" class=\"data row18 col3\" >1.00</td>\n",
       "      <td id=\"T_06eb6_row18_col4\" class=\"data row18 col4\" >mentions being sick or having symptoms of illness</td>\n",
       "      <td id=\"T_06eb6_row18_col5\" class=\"data row18 col5\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_06eb6_level0_row19\" class=\"row_heading level0 row19\" >1</th>\n",
       "      <td id=\"T_06eb6_row19_col0\" class=\"data row19 col0\" >17</td>\n",
       "      <td id=\"T_06eb6_row19_col1\" class=\"data row19 col1\" >-0.258996</td>\n",
       "      <td id=\"T_06eb6_row19_col2\" class=\"data row19 col2\" >expresses sadness or sympathy</td>\n",
       "      <td id=\"T_06eb6_row19_col3\" class=\"data row19 col3\" >0.98</td>\n",
       "      <td id=\"T_06eb6_row19_col4\" class=\"data row19 col4\" >expresses sadness or empathy for another's distress</td>\n",
       "      <td id=\"T_06eb6_row19_col5\" class=\"data row19 col5\" >0.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f3a26608c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This cell is entirely borrowed from detailed_usage.ipynb.\n",
    "\n",
    "scoring_config = ScoringConfig(\n",
    "    n_examples=50,\n",
    "    max_words_per_example=128,\n",
    ")\n",
    "\n",
    "all_metrics = interpreter.score_interpretations(\n",
    "    texts=train_texts,\n",
    "    activations=train_activations,\n",
    "    interpretations=interpretations,\n",
    "    config=scoring_config,\n",
    ")\n",
    "\n",
    "interpretations_data = []\n",
    "for neuron_idx in selected_neurons:\n",
    "    neuron_metrics = all_metrics[neuron_idx]\n",
    "    best_interp, best_metrics = max(neuron_metrics.items(), key=lambda x: x[1]['f1'])\n",
    "    worst_interp, worst_metrics = min(neuron_metrics.items(), key=lambda x: x[1]['f1'])\n",
    "    \n",
    "    interpretations_data.append({\n",
    "        'neuron_idx': neuron_idx,\n",
    "        f'target_{selection_method}': scores[selected_neurons.index(neuron_idx)],\n",
    "        'best_interpretation': best_interp,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'worst_interpretation': worst_interp,\n",
    "        'worst_f1': worst_metrics['f1']\n",
    "    })\n",
    "\n",
    "best_interp_df = pd.DataFrame(interpretations_data).sort_values(by=f'target_{selection_method}', ascending=False)\n",
    "\n",
    "display(\n",
    "    best_interp_df.style.format({\n",
    "        'separation_score': '{:.2f}',\n",
    "        'best_f1': '{:.2f}', \n",
    "        'worst_f1': '{:.2f}'\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55c382",
   "metadata": {},
   "source": [
    "Lastly, the best interpretations for each neuron are used to annotate our holdout set, with each concept scored as a hypothesis for predicting `sentiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6236bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_holdout_texts = val_df_holdout['text']\n",
    "val_holdout_sentiments = val_df_holdout['sentiment']\n",
    "\n",
    "holdout_annotations = annotate_texts_with_concepts(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    texts=val_holdout_texts,\n",
    "    concepts=best_interp_df['best_interpretation'].tolist(),\n",
    "    max_words_per_example=128,\n",
    "    cache_name=CACHE_NAME,\n",
    "    n_workers=50,\n",
    ")\n",
    "\n",
    "metrics, hypothesis_df = score_hypotheses(\n",
    "    hypothesis_annotations=holdout_annotations,\n",
    "    y_true=val_holdout_sentiments,\n",
    "    classification=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "559ebdca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>separation_score</th>\n",
       "      <th>separation_pval</th>\n",
       "      <th>regression_coef</th>\n",
       "      <th>regression_pval</th>\n",
       "      <th>feature_prevalence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>expresses strong positive emotion</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>expresses gratitude</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mentions Mother's Day or mothers</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>expresses love towards another person</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uses the word 'cute' to describe someone or something</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>uses the word 'morning'</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mentions creating or promoting content/products</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uses an exclamation mark at the end of the tweet</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>expresses a feeling of longing or missing someone or something</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mentions a physical ailment of the head</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>expresses a state of being tired</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mentions discomfort in the stomach</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>expresses apology</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mentions being sick or having symptoms of illness</td>\n",
       "      <td>-0.629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mentions the word 'missed'</td>\n",
       "      <td>-0.706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.612</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>expresses sadness or sympathy</td>\n",
       "      <td>-0.723</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>uses the word 'bored' or variations thereof</td>\n",
       "      <td>-0.744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>uses the word 'sucks'</td>\n",
       "      <td>-0.806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>expresses sadness</td>\n",
       "      <td>-0.806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>expresses strong negative emotion, specifically hate or dislike</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.430</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         hypothesis  \\\n",
       "0                                 expresses strong positive emotion   \n",
       "1                                               expresses gratitude   \n",
       "2                                  mentions Mother's Day or mothers   \n",
       "4                             expresses love towards another person   \n",
       "7             uses the word 'cute' to describe someone or something   \n",
       "6                                           uses the word 'morning'   \n",
       "5                   mentions creating or promoting content/products   \n",
       "3                  uses an exclamation mark at the end of the tweet   \n",
       "17   expresses a feeling of longing or missing someone or something   \n",
       "10                          mentions a physical ailment of the head   \n",
       "11                                 expresses a state of being tired   \n",
       "8                                mentions discomfort in the stomach   \n",
       "15                                                expresses apology   \n",
       "18                mentions being sick or having symptoms of illness   \n",
       "12                                       mentions the word 'missed'   \n",
       "19                                    expresses sadness or sympathy   \n",
       "14                      uses the word 'bored' or variations thereof   \n",
       "13                                            uses the word 'sucks'   \n",
       "9                                                 expresses sadness   \n",
       "16  expresses strong negative emotion, specifically hate or dislike   \n",
       "\n",
       "    separation_score  separation_pval  regression_coef  regression_pval  \\\n",
       "0              0.857              0.0            0.602            0.000   \n",
       "1              0.835              0.0            0.373            0.000   \n",
       "2              0.567              0.0            0.094            0.018   \n",
       "4              0.535              0.0            0.088            0.005   \n",
       "7              0.518              0.0            0.168            0.029   \n",
       "6              0.391              0.0            0.276            0.000   \n",
       "5              0.368              0.0            0.114            0.000   \n",
       "3              0.210              0.0           -0.045            0.010   \n",
       "17            -0.318              0.0           -0.027            0.323   \n",
       "10            -0.393              0.0           -0.026            0.670   \n",
       "11            -0.418              0.0           -0.125            0.000   \n",
       "8             -0.608              0.0           -0.124            0.170   \n",
       "15            -0.616              0.0           -0.370            0.000   \n",
       "18            -0.629              0.0           -0.152            0.000   \n",
       "12            -0.706              0.0           -0.612            0.000   \n",
       "19            -0.723              0.0           -0.150            0.000   \n",
       "14            -0.744              0.0           -0.378            0.000   \n",
       "13            -0.806              0.0           -0.121            0.100   \n",
       "9             -0.806              0.0           -0.187            0.000   \n",
       "16            -0.833              0.0           -0.430            0.000   \n",
       "\n",
       "    feature_prevalence  \n",
       "0                0.376  \n",
       "1                0.084  \n",
       "2                0.044  \n",
       "4                0.084  \n",
       "7                0.010  \n",
       "6                0.029  \n",
       "5                0.101  \n",
       "3                0.302  \n",
       "17               0.125  \n",
       "10               0.018  \n",
       "11               0.113  \n",
       "8                0.007  \n",
       "15               0.025  \n",
       "18               0.060  \n",
       "12               0.033  \n",
       "19               0.211  \n",
       "14               0.012  \n",
       "13               0.011  \n",
       "9                0.195  \n",
       "16               0.179  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Holdout Set Metrics:\n",
      "R² Score: 0.481\n",
      "Significant hypotheses: 13/20 (p < 5.000e-03)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "display(hypothesis_df.round(3))\n",
    "pd.reset_option('display.max_colwidth')\n",
    "\n",
    "print(\"\\nHoldout Set Metrics:\")\n",
    "print(f\"R² Score: {metrics['r2']:.3f}\")\n",
    "print(f\"Significant hypotheses: {metrics['Significant'][0]}/{metrics['Significant'][1]} \" \n",
    "      f\"(p < {metrics['Significant'][2]:.3e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f53f0b",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "I had some time left to dive deeper into the project, so I decided to tinker around and implement two new approaches: **hypothesis annotation by embedding space metrics** and **the `SupervisedSparseAutoencoder` class specified [here](https://github.com/rmovva/HypotheSAEs/issues/2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac1d17",
   "metadata": {},
   "source": [
    "## i. Hypothesis Annotation by Embedding Space Metrics\n",
    "This was a suggestion in the spec for this project and I found it pretty interesting. The code below uses two approaches for discriminating the holdout dataset's text embeddings wrt the hypotheses: distance and cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d12fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c1ce37151e4700b984c26040e84a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading embedding chunks:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 27460 embeddings in 0.1s\n",
      "Loaded model nomic-ai/modernbert-embed-base to cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e7397438f742e0a1bd7b5c26ec9ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cf03c530404b6c8d0547a3eedd3a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk 2:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 13 embeddings to /home/phoenixw/HypotheSAEs/emb_cache/twitter_quickstart_local_nomic-ai/modernbert-embed-base/chunk_002.npy\n"
     ]
    }
   ],
   "source": [
    "val_holdout_texts = val_df_holdout['text'].tolist()\n",
    "hypotheses_texts = best_interp_df['best_interpretation'].tolist()\n",
    "\n",
    "_ = get_local_embeddings(val_holdout_texts + hypotheses_texts, model=EMBEDDER, batch_size=128, cache_name=CACHE_NAME)\n",
    "val_holdout_embeddings = np.stack([_[text] for text in val_holdout_texts])\n",
    "hypothesis_embeddings = np.stack([_[text] for text in hypotheses_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96971fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_cosine(examples : np.ndarray, hypothesis : np.ndarray, k : float = 0.7):\n",
    "    \"\"\"\n",
    "    Returns the indices of the most cosine-similar examples to the concept, as indexed by examples.\n",
    "    \"\"\"\n",
    "    similarity_scores = examples @ hypothesis / (np.linalg.norm(examples, axis=1) * np.linalg.norm(hypothesis))\n",
    "    indices = np.where(similarity_scores >= k)[0] # GPT-Generated!\n",
    "    if indices.size == 0:\n",
    "        return [int(np.argmax(similarity_scores))]\n",
    "    return indices\n",
    "\n",
    "def score_dist(examples : np.ndarray, hypothesis : np.ndarray, r : float = 4):\n",
    "    \"\"\"\n",
    "    Returns the indices of the most distance-similar examples to the concept, as indexed by examples.\n",
    "    \"\"\"\n",
    "\n",
    "    diffs = examples - hypothesis\n",
    "    dists = np.linalg.norm(diffs, axis=1)\n",
    "\n",
    "    indices = np.where(dists <= r)[0]\n",
    "    if indices.size == 0:\n",
    "        return [int(np.argmin(dists))]\n",
    "    return indices\n",
    "\n",
    "def annotate_by_embedding(metric, examples : np.ndarray, hypotheses : np.ndarray, **kwargs):\n",
    "    \"\"\"\n",
    "    Annotate all examples with all hypotheses.\n",
    "\n",
    "    Args:\n",
    "        examples : list of text embeddings\n",
    "        hypotheses : list of hypothesis embeddings\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping a hypothesis to its one-hot annotation results for each example.\n",
    "    \"\"\"\n",
    "    annotations: dict[str, np.ndarray] = {}\n",
    "    # concept -> annotation results for each example\n",
    "    for i, hypothesis in enumerate(hypotheses):\n",
    "        best_example_indices = metric(examples, hypothesis, **kwargs)\n",
    "        one_hot = np.zeros(len(examples), dtype=int)\n",
    "        one_hot[best_example_indices] = 1\n",
    "        annotations[hypotheses_texts[i]] = one_hot\n",
    "    return annotations\n",
    "\n",
    "annotations_by_cosine = annotate_by_embedding(score_cosine, val_holdout_embeddings, hypothesis_embeddings, k=0.6)\n",
    "annotations_by_dist = annotate_by_embedding(score_dist, val_holdout_embeddings, hypothesis_embeddings, r=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e9f78789",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_cosine, hypothesis_df_cosine = score_hypotheses(\n",
    "    hypothesis_annotations=annotations_by_cosine,\n",
    "    y_true=val_holdout_sentiments,\n",
    "    classification=False,\n",
    ")\n",
    "\n",
    "metrics_dist, hypothesis_df_dist = score_hypotheses(\n",
    "    hypothesis_annotations=annotations_by_dist,\n",
    "    y_true=val_holdout_sentiments,\n",
    "    classification=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "99e54132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Metric\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>separation_score</th>\n",
       "      <th>separation_pval</th>\n",
       "      <th>regression_coef</th>\n",
       "      <th>regression_pval</th>\n",
       "      <th>feature_prevalence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>expresses gratitude</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>expresses strong positive emotion</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uses the word 'cute' to describe someone or something</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>expresses love towards another person</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mentions creating or promoting content/products</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mentions Mother's Day or mothers</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uses an exclamation mark at the end of the tweet</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>uses the word 'morning'</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>expresses apology</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>expresses sadness</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mentions discomfort in the stomach</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>expresses sadness or sympathy</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>expresses a feeling of longing or missing someone or something</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>uses the word 'bored' or variations thereof</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mentions the word 'missed'</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mentions a physical ailment of the head</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>expresses a state of being tired</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>uses the word 'sucks'</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mentions being sick or having symptoms of illness</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>expresses strong negative emotion, specifically hate or dislike</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         hypothesis  \\\n",
       "1                                               expresses gratitude   \n",
       "0                                 expresses strong positive emotion   \n",
       "7             uses the word 'cute' to describe someone or something   \n",
       "4                             expresses love towards another person   \n",
       "5                   mentions creating or promoting content/products   \n",
       "2                                  mentions Mother's Day or mothers   \n",
       "3                  uses an exclamation mark at the end of the tweet   \n",
       "6                                           uses the word 'morning'   \n",
       "15                                                expresses apology   \n",
       "9                                                 expresses sadness   \n",
       "8                                mentions discomfort in the stomach   \n",
       "19                                    expresses sadness or sympathy   \n",
       "17   expresses a feeling of longing or missing someone or something   \n",
       "14                      uses the word 'bored' or variations thereof   \n",
       "12                                       mentions the word 'missed'   \n",
       "10                          mentions a physical ailment of the head   \n",
       "11                                 expresses a state of being tired   \n",
       "13                                            uses the word 'sucks'   \n",
       "18                mentions being sick or having symptoms of illness   \n",
       "16  expresses strong negative emotion, specifically hate or dislike   \n",
       "\n",
       "    separation_score  separation_pval  regression_coef  regression_pval  \\\n",
       "1              0.304            0.000            0.397            0.000   \n",
       "0              0.289            0.000            0.425            0.000   \n",
       "7              0.284            0.000            0.184            0.000   \n",
       "4              0.246            0.000            0.160            0.000   \n",
       "5              0.135            0.000            0.063            0.049   \n",
       "2              0.079            0.000            0.058            0.012   \n",
       "3              0.047            0.058            0.061            0.023   \n",
       "6             -0.000            0.992            0.069            0.005   \n",
       "15            -0.023            0.281           -0.067            0.010   \n",
       "9             -0.100            0.000           -0.224            0.000   \n",
       "8             -0.103            0.000           -0.008            0.744   \n",
       "19            -0.123            0.000           -0.091            0.004   \n",
       "17            -0.125            0.000           -0.172            0.000   \n",
       "14            -0.139            0.000           -0.003            0.917   \n",
       "12            -0.144            0.000           -0.147            0.000   \n",
       "10            -0.160            0.000           -0.037            0.170   \n",
       "11            -0.211            0.000           -0.094            0.001   \n",
       "13            -0.277            0.000           -0.225            0.000   \n",
       "18            -0.290            0.000           -0.170            0.000   \n",
       "16            -0.525            0.000           -0.431            0.000   \n",
       "\n",
       "    feature_prevalence  \n",
       "1                0.546  \n",
       "0                0.317  \n",
       "7                0.149  \n",
       "4                0.175  \n",
       "5                0.112  \n",
       "2                0.426  \n",
       "3                0.224  \n",
       "6                0.610  \n",
       "15               0.437  \n",
       "9                0.443  \n",
       "8                0.524  \n",
       "19               0.224  \n",
       "17               0.140  \n",
       "14               0.299  \n",
       "12               0.385  \n",
       "10               0.430  \n",
       "11               0.348  \n",
       "13               0.390  \n",
       "18               0.249  \n",
       "16               0.056  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Similarity Metric\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>separation_score</th>\n",
       "      <th>separation_pval</th>\n",
       "      <th>regression_coef</th>\n",
       "      <th>regression_pval</th>\n",
       "      <th>feature_prevalence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uses the word 'cute' to describe someone or something</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mentions Mother's Day or mothers</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>expresses gratitude</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>expresses love towards another person</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>expresses strong positive emotion</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mentions creating or promoting content/products</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>uses the word 'morning'</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uses an exclamation mark at the end of the tweet</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>expresses apology</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.938</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>expresses strong negative emotion, specifically hate or dislike</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>expresses a feeling of longing or missing someone or something</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>expresses sadness</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>expresses a state of being tired</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>uses the word 'bored' or variations thereof</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>uses the word 'sucks'</td>\n",
       "      <td>-0.606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mentions discomfort in the stomach</td>\n",
       "      <td>-0.611</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mentions the word 'missed'</td>\n",
       "      <td>-0.669</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mentions a physical ailment of the head</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mentions being sick or having symptoms of illness</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>expresses sadness or sympathy</td>\n",
       "      <td>-0.812</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         hypothesis  \\\n",
       "7             uses the word 'cute' to describe someone or something   \n",
       "2                                  mentions Mother's Day or mothers   \n",
       "1                                               expresses gratitude   \n",
       "4                             expresses love towards another person   \n",
       "0                                 expresses strong positive emotion   \n",
       "5                   mentions creating or promoting content/products   \n",
       "6                                           uses the word 'morning'   \n",
       "3                  uses an exclamation mark at the end of the tweet   \n",
       "15                                                expresses apology   \n",
       "16  expresses strong negative emotion, specifically hate or dislike   \n",
       "17   expresses a feeling of longing or missing someone or something   \n",
       "9                                                 expresses sadness   \n",
       "11                                 expresses a state of being tired   \n",
       "14                      uses the word 'bored' or variations thereof   \n",
       "13                                            uses the word 'sucks'   \n",
       "8                                mentions discomfort in the stomach   \n",
       "12                                       mentions the word 'missed'   \n",
       "10                          mentions a physical ailment of the head   \n",
       "18                mentions being sick or having symptoms of illness   \n",
       "19                                    expresses sadness or sympathy   \n",
       "\n",
       "    separation_score  separation_pval  regression_coef  regression_pval  \\\n",
       "7              0.905            0.000            0.778            0.000   \n",
       "2              0.647            0.000            0.497            0.000   \n",
       "1              0.638            0.000            0.639            0.000   \n",
       "4              0.413            0.012            0.116            0.487   \n",
       "0              0.384            0.000            0.375            0.000   \n",
       "5              0.157            0.649            0.167            0.611   \n",
       "6              0.089            0.016            0.062            0.094   \n",
       "3              0.046            0.625            0.052            0.560   \n",
       "15            -0.005            0.938           -0.152            0.037   \n",
       "16            -0.043            0.938            0.560            0.288   \n",
       "17            -0.316            0.175           -0.174            0.458   \n",
       "9             -0.431            0.000           -0.416            0.000   \n",
       "11            -0.498            0.000           -0.222            0.001   \n",
       "14            -0.558            0.000           -0.420            0.000   \n",
       "13            -0.606            0.000           -0.259            0.003   \n",
       "8             -0.611            0.000           -0.371            0.000   \n",
       "12            -0.669            0.000           -0.680            0.000   \n",
       "10            -0.690            0.000           -0.195            0.033   \n",
       "18            -0.700            0.000           -0.184            0.073   \n",
       "19            -0.812            0.000           -0.159            0.301   \n",
       "\n",
       "    feature_prevalence  \n",
       "7                0.003  \n",
       "2                0.036  \n",
       "1                0.044  \n",
       "4                0.004  \n",
       "0                0.012  \n",
       "5                0.001  \n",
       "6                0.088  \n",
       "3                0.012  \n",
       "15               0.024  \n",
       "16               0.000  \n",
       "17               0.002  \n",
       "9                0.020  \n",
       "11               0.028  \n",
       "14               0.009  \n",
       "13               0.017  \n",
       "8                0.031  \n",
       "12               0.007  \n",
       "10               0.019  \n",
       "18               0.013  \n",
       "19               0.006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Holdout Set Metrics:\n",
      "R² Score (cosine): 0.216\n",
      "R² Score (distance): 0.099\n",
      "Significant hypotheses: \n",
      "      (cosine) 12/20 (p < 5.000e-03)\n",
      "      (distance) 10/20 \" (p < 5.000e-03)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "print(\"Cosine Similarity Metric\")\n",
    "display(hypothesis_df_cosine.round(3))\n",
    "print(\"Distance Similarity Metric\")\n",
    "display(hypothesis_df_dist.round(3))\n",
    "pd.reset_option('display.max_colwidth')\n",
    "\n",
    "print(\"\\nHoldout Set Metrics:\")\n",
    "print(f\"R² Score (cosine): {metrics_cosine['r2']:.3f}\")\n",
    "print(f\"R² Score (distance): {metrics_dist['r2']:.3f}\")\n",
    "print(f\"\"\"Significant hypotheses: \n",
    "      (cosine) {metrics_cosine['Significant'][0]}/{metrics_cosine['Significant'][1]} (p < {metrics_cosine['Significant'][2]:.3e})\n",
    "      (distance) {metrics_dist['Significant'][0]}/{metrics_dist['Significant'][1]} \" (p < {metrics_dist['Significant'][2]:.3e})\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c168ded3",
   "metadata": {},
   "source": [
    "It seems that the same trends are invariant to the mode of annotation here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f71b85f",
   "metadata": {},
   "source": [
    "## ii. SupervisedSparseAutoencoder Class\n",
    "Motivating idea: what if we learned a sparse representation that was not only helpful for reconstruction, but also for predicting the target variable? Unfortunately, this is where I ran out of time. I have implemented the constructor and forward pass (to the best of my knowledge), but did not have time to finish the rest of the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "77503aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class SupervisedSparseAutoencoder(SparseAutoencoder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        m_total_neurons: int,\n",
    "        k_active_neurons: int,\n",
    "        supervised_dim=1,\n",
    "        alpha=1.0,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        supervised_dim: dimension of target y (1 for scalar regression)\n",
    "        alpha: weight of supervised loss relative to reconstruction\n",
    "        \"\"\"\n",
    "        super().__init__(input_dim, m_total_neurons, k_active_neurons, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.supervised_dim = supervised_dim\n",
    "\n",
    "        # projector on latent space to target prediction\n",
    "        self.supervised_head = nn.Linear(self.m_total_neurons, supervised_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, dict]:\n",
    "        recon, info = super().forward(x)\n",
    "\n",
    "        latent_code = info['activations']\n",
    "        y_prediction = self.supervised_head(latent_code)\n",
    "        return recon, y_prediction, info\n",
    "    \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        recon: torch.Tensor,\n",
    "        info: dict[str, torch.Tensor],\n",
    "        aux_coef: float,\n",
    "        multi_coef: float,\n",
    "    ) -> torch.Tensor:\n",
    "        pass\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        X_train: torch.Tensor,\n",
    "        X_val: Optional[torch.Tensor] = None,\n",
    "        save_dir: Optional[str] = None,\n",
    "        batch_size: int = 512,\n",
    "        learning_rate: float = 5e-4,\n",
    "        n_epochs: int = 200,\n",
    "        aux_coef: float = 1 / 32,\n",
    "        multi_coef: float = 0.0,\n",
    "        patience: int = 5,\n",
    "        show_progress: bool = True,\n",
    "        clip_grad: float = 1.0\n",
    "    ) -> dict:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HSAEs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
